WGAN - Wasserstein Generative Adversarial Network with Gradient Penalty

Overview
This repository implements a Wasserstein Generative Adversarial Network (WGAN) with a gradient penalty for stable training. WGANs are a type of GAN that improves training stability and quality by using the Wasserstein distance (Earth Mover's Distance) instead of the traditional Jensen-Shannon divergence.

The project includes the training of both the generator and the discriminator using a range of hyperparameters. The final trained model is used to generate synthetic data that closely resembles a given dataset.

Features
WGAN with Gradient Penalty (WGAN-GP): The training of the generator and discriminator is improved using the gradient penalty to enforce the Lipschitz constraint.

Grid Search for Hyperparameter Tuning: The hyperparameters of the GAN, such as learning rate, batch size, number of epochs, noise dimensions, and gradient penalty weight, are optimized using a grid search approach.

Data Generation: After training, the generator is used to generate synthetic data that resembles the original dataset.

Main Functions
train(): This function runs the training process for the GAN using a range of hyperparameters. It saves the models (generator and discriminator) after each epoch.

generate_data(): Generates synthetic data using the trained generator. This data is rescaled to match the original dataset.

load_models(): Loads the pre-trained generator and discriminator models.

discriminator_loss(): Defines the loss function for the discriminator, including the Wasserstein loss and gradient penalty.

generator_loss(): Defines the loss function for the generator, which is the negative of the discriminator’s output.

gradient_penalty(): Computes the gradient penalty, which enforces the Lipschitz continuity on the discriminator.

Hyperparameters for Grid Search
The following hyperparameters are used for grid search during training:

learning_rates: c(0.001, 0.0001) - Learning rates for the optimizer.

batch_sizes: c(32, 64) - Batch sizes for training.

epochs: c(20, 30) - Number of epochs to train the model.

d_steps: 3 - Number of steps to train the discriminator per generator update.

noise_dim: 50 - The dimension of the random noise vector fed to the generator.

gp_weights: 5 - Weight for the gradient penalty in the discriminator's loss.

Training Process
The training loop consists of the following steps:

Discriminator Training: The discriminator is trained with real data and fake data generated by the generator. The gradient penalty is added to enforce the Lipschitz constraint.

Generator Training: The generator is updated by maximizing the discriminator’s output for fake data, effectively improving its ability to generate realistic data.

Contributing
If you would like to contribute to this project, please fork the repository and create a pull request. Contributions, bug reports, and feature requests are always welcome!

License
This project is licensed under the MIT License - see the LICENSE file for details.

